# Question 1.1 - Data ingestion
Spond has multiple sources of data that pertain to different areas of the business. Generally speaking, the three main categories are 1) internal application data, 2) external vendor data, and 3) other third party data.

1. The internal application data can be assumed to be stored at a distributed set of RDS instances in AWS. This data mainly relates to data generated from the applications that Spond provides to end-users. Examples of datasets include payments, profiles, groups, clubs, etc.

2. External vendor data are data that are stored with external service providers that relate to different business areas in Spond. Some are user-facing (e.g., Intercom for product support and Typeform for user surveys), while others are used as internal tooling (e.g., CRM systems, accounting systems, etc.). The format and availability of the data vary by vendor.

3. Other third party data primarily concern datasets that are needed for intermediate enrichments of existing data. Examples include foreign exchange rates, geolocation mapping, public demographics data, etc. The format and availability of the data vary by vendor - but are mostly API based.

Your goal is to design a robust set of data ingestion pipelines with the purpose of extracting data from the source system. Please detail the technology stack and tools you would utilize to extract and ingest data from these diverse sources into our system. In your response, describe your strategy for handling the different data formats and availability across these sources. Additionally, elaborate on your approach to optimizing this data ingestion process in terms of both performance and cost. Consider aspects such as handling data in real-time versus batch processing, dealing with large data volumes, and potential network issues.

---

## Response

NOTE: I am aware that Spond already uses Databricks and AWS, so I will work on that assumption.
I would have chosen them as well. If you'd like to know why I think so, I'd be happy to have a chat to discuss it.

### Data ingestion

There are two main types of data ingestion to consider

#### 1. Data store ingestion
*This is the type of ingestion where you can directly access the full dataset and are able to select which data to pull.
This is the most common way to ingest data, and often the most efficient.*

When dealing with data store ingestion, the ideal scenario is that the source system can support incremental loading.
This means that it has a marker suited for the ingestion use case, and it allows reading data selectively based on that
marker. An example would be an append-only database table with a column showing the creation timestamp, or a
*current-state* table with a column showing the last time each row was edited.

In this scenario, the best option would be to create a connection to the database, and select data using push-down
queries so that only the required data is transferred over the network, rather than returning everything and filtering
on the data platform's side. This approach would require having a mechanism to identify the latest successful run so
that it is possible to identify what data needs to be read. If the load runs at a fixed time interval, and the source
data store guarantees timely delivery, then a simple run-time based filter would be sufficient. Alternatively, a logging
table can be kept and updated when a run is successful, allowing the next run to retrieve that information.

Nevertheless, nothing is always perfect and there are cases where the datastore just doesn't support incremental
loading. In this case the only option would be to read the dataset in full on every run. There is sometimes a
possibility where the data store maintainers can provide some method of notification when the data is updated; this
would be useful so that the ingestion pipeline is only run when needed, rather than processing the full dataset at a
scheduled interval, even when it's not required.

In either case, Databricks/Spark supports reading from a variety of data sources and formats and is a perfect candidate
for this.

#### 2. API-based ingestion
*This is the type of ingestion where you don't have access to the data directly, but you have to
retrieve it through a set of APIs*

Similarly to data store ingestion, some APIs will provide the ability for incremental loading, although, in my
experience, the majority won't.

Building API-based ingestion can be a laborious process with little ROI. I imagine many people agree with this
considering the recent boom in data integration tools like Fivetran, Rivery, Airbyte, and others.

I do have concerns over these platforms though as I find many of them extremely expensive for the service they provide.
I also sometimes find that their enforced data loading patterns are unsuitable or seemingly deliberately designed to
inflate the amount of data being transferred.

In cases where the necessary data can be retrieved from only a few API endpoints, I believe a purpose-built script
doesn't take very long to create and will serve the purpose it's needed for.

Nevertheless, when the goal is to ingest all or most data from the source, then a data integration tool might come in
handy.

I find Airbyte to be one of the best options out there as it's open-source, allowing users to further develop it or
even add new integrations, and it can be self-hosted. This would allow an easy switch over from managed to self-hosted
once the costs of the managed service exceed those of the self-hosted one.

### Data Format

I prefer using Delta Lake when working with Spark and/or Databricks.
It combines the compression and battle-testedness of Parquet with the flexibility of an RDBMS, without compromising on
speed on storage.

### Optimisation
When working with the combination of Databricks, Spark, Delta Lake and Parquet there are certain measures I like to take

1. Ensuring scheduled workloads are only run at the minimum frequency needed. In most cases, there's no need for the
workloads to run over the weekend, for example.
2. Designing the pipelines to be incremental and not process data in full on each run
3. Frequently vacuum tables when working with Delta Lake to avoid history buildup which would increase storage costs
4. Set the below spark settings as default

    | Spark Conf                                            | Value                                                        | Reasoning                                                                                                                                                 |
    |-------------------------------------------------------|--------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|
    | spark.serializer                                      | org.apache.spark.serializer.KryoSerializer                   | Serialization is at the core of Spark, and the Kryo serializer is much more optimised and significantly faster than the standard Java serializer          |
    | spark.executor.extraJavaOptions:                      | -XX:+UseG1GC                                                 | Use G1GC garbage collection to improve performance                                                                                                        |
    | spark.databricks.io.cache.enabled                     | true                                                         | Enable disk cache to accelerate reading data                                                                                                              |
    | spark.databricks.adaptive.autoOptimizeShuffle.enabled | true                                                         | Lets Databricks automatically manage the number of shuffle partitions and adjust it throughout the job                                                    |
    | spark.databricks.delta.optimizeWrite.enabled          | true                                                         | Lets Databricks manage the file size to optimise the number of files written to each partition                                                            |
    | spark.databricks.delta.autoCompact.enabled            | auto                                                         | Allow Databricks to automatically combine multiple small files into one for better read performance                                                       |
    | spark.sql.streaming.stateStore.providerClass          | com.databricks.sql.streaming.state.RocksDBStateStoreProvider | An optimised state management for streaming pipelines                                                                                                     |
    | spark.sql.streaming.forceDeleteTempCheckpointLocation | true                                                         | Spark Streaming checkpoints can take up a surprisingly large amount of space, this ensures that Spark deletes any temp checkpoints to reduce storage cost |
    | spark.checkpoint.compress                             | true                                                         | Unless extremely high throughput is necessary, then compressing the checkpoint files will save a lot of storage space                                     |
    | spark.cleaner.referenceTracking.cleanCheckpoints      | true                                                         | Cleaning checkpoint files if they're out of scope to reduce unnecessary storage                                                                           |


### Batch vs. Streaming
I always assume batch is the default and streaming is the exception for the following reasons

1. Up until data hits a certain volume, batch is always cheaper than streaming, even with the smallest cluster possible
(unless it's free...)
2. Designing and maintaining a streaming pipeline takes more time which could be spent on something else if it's not
necessary
3. The vast majority of analytics use cases do not require data to be in real-time

With that said, there is indeed room for streaming pipelines despite the points above.

As mentioned, streaming pipelines are more work to build and maintain. However, Delta Lake, when combined with Spark,
provides some very good functionality for optimising performance and simplifying development

For example, reading from a Delta Table in a streaming fashion becomes very cost-efficient as Delta tables emit a useful
log with all the changes happening to the table. When streaming from them in Spark, the log is used as the trigger for
updates rather than scanning the data.

It's also really simple to configure idempotent writes with Delta tables; a simple setting with a batch ID will make
sure the data doesn't get written twice!

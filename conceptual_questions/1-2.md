# Question 1.2 - Data transformation and cleaning
Spond has multiple sources of data that pertain to different areas of the business. Generally speaking, the three main categories are 1) internal application data, 2) external vendor data, and 3) other third party data.

1. The internal application data can be assumed to be stored at a distributed set of RDS instances in AWS. This data mainly relates to data generated from the applications that Spond provides to end-users. Examples of datasets include payments, profiles, groups, clubs, etc.

2. External vendor data are data that are stored with external service providers that relate to different business areas in Spond. Some are user-facing (e.g., Intercom for product support and Typeform for user surveys), while others are used as internal tooling (e.g., CRM systems, accounting systems, etc.). The format and availability of the data vary by vendor.

3. Other third party data primarily concern datasets that are needed for intermediate enrichments of existing data. Examples include foreign exchange rates, geolocation mapping, public demographics data, etc. The format and availability of the data vary by vendor - but are mostly API based.

Once the data is ingested and stored, it's often required to transform and clean the data to make it suitable for downstream applications like analytics and machine learning. Describe your approach to data transformations and cleaning. What tools or frameworks would you use for this task? How would you handle schema evolution and ensure data quality? Please give examples of some common data quality issues and how you would address them.

---

## Response

### Approach

I always work with two assumptions when transforming and cleaning data
1. Something ***will*** go wrong eventually
2. There is no one-size-fits-all

With that in mind, I prefer splitting transformation into smaller chunks rather than one big pipeline that takes data in
and spits data out. Specifically, separating data cleansing, standard transformations, and business logic
transformations, in that order.

It is all but certain that business logic will change; once that happens, it is much easier and more cost-efficient to
only rerun the last part of the pipeline rather than re-ingesting all raw data and starting from scratch.

Similarly, standard transformations can change or be added to, although less frequently than business logic changes.
For example, adding a column to record the data the first time a value was observed.

Finally, data cleansing is almost static, unless new data comes in. Once the pipeline is production-ready, it's
uncommon, but not unheard of, to have to go back and change that again.

This approach provides flexibility, recoverability, and cost efficiency all in one which is why I prefer it.

As for the shape of the produced data, I prefer producing the data in multiple formats to cater for a variety of use
cases. Namely,

1. An OBT (One Big Table) layer that caters for data science, and single-view use cases
2. A Kimball/Inmon style layer that allows for standard BI and analytics with high performance
3. An aggregates layer that serves the less data-savvy with summarised data regarding key information

### Tools & Frameworks
I find Python to be a very flexible language with a wealth of external libraries and community support that makes it a
great candidate for performing data transformations. While SQL is an indispensable tool/language when dealing with data,
it lacks the flexibility and parameterizability of Python. dbt is a very intelligent way to solve that issue, but I
personally find that it can get very messy very quickly, regardless of how well-structured a project is. That is not
to say it's not a useful tool, but I personally wouldn't recommend building a whole data platform based solely on dbt.

PySpark is a suitable alternative framework. It provides the scalability needed for any size of data, and is easy to
keep nice and tidy by tapping into the Python ecosystem without extra work.

That is not to say it doesn't have downsides. Spark, especially on Databricks, is not great for working with very small
data. If Excel can do it, Spark probably shouldn't do it.... A simpler framework like Pandas or Polars would be more
suited for smaller datasets. I personally prefer Polars as it has a similar API to PySpark, and is orders of magnitude
more efficient than Pandas.

However, running non-Spark workloads on Databricks can be very wasteful. A Databricks cluster takes ~5 minutes to start,
and that doesn't make much sense when the workload can be completed in less than that.

Ideally, there would be an orchestrator available that can run Spark workloads on Databricks, and non-Spark workloads on
its own runners, for example Airflow or Dagster. Alternatively, simply using an AWS lambda to run the code might be more
cost-efficient than running it on Databricks.

To summarise, my choice of tooling and framework would be
- Databricks + PySpark for large workloads
- Polars + Python environment for smaller workloads
- Airflow or Dagster for orchestration

### Schema Evolution
When it comes to schema evolution my view is that it must be very strict.

- All tables should be defined and created explicitly, not implicitly through writing dataframes for example
- No `select *`s should be used when writing data to ensure that it's a controlled process
- Monitoring should be in place to alert on observed schema changes allowing the developers to address it as necessary.
The default behaviour of the code should be to either ignore them or stop the process
- There should be a defined, easy-to-use process for changing the schema and testing it before the change is implemented
- If the source system is internal, coordination between the data producers and consumers is paramount

### Data Quality
#### Approach
When it comes to data quality, I find the current landscape a little lacking. Writing DQ checks within a pipeline feels
like a distraction from the main purpose, while not writing them at all feels like driving with a blindfold. Databricks
provides a very nice solution to this, but it's only available for DLT pipelines which is a vendor lock-in product that
I'm wary of. Then there's the almost industry-standard Great Expectations framework which arguably has one of the most
developer-unfriendly APIs in the Python ecosystem, not unlike its AWS alternative, PyDeequ.

There is, of course, a number of paid tools that can be used such as Monte Carlo, Anomalo, etc. Recently, I've been
looking into [Soda](https://github.com/sodadata/soda-core) which I think has very good potential. I like that it has
similar versatility to dbt tests, without coupling them to a specific framework or tool.

Generally, I think data quality is an integral part of the pipeline development process. I would expect each pipeline to
be coupled with unit tests to ensure it produces the correct data. In addition, there should be data tests that can run
on the data before writing and periodically if necessary.

My preferred approach is the Write-Audit-Publish pattern for the following reasons
- It guarantees valid data checks. Running DQ checks on an in-memory dataframe doesn't guarantee that the output data is
correct as there could be changes to the data on write.
- It decouples the processing logic from the DQ logic and treats them as two different stages, which I believe they
should be
- It provides the ability for developers to immediately and inexpensively investigate the data on errors, rather than
having to rerun the pipeline and hope the error is reproduced

#### Common Issues
- Invalid data (phone numbers, addresses, emails)
  - **Issue**: Fields having invalid data in them such as an email without a domain, a phone number that's too short, a
  post code that doesn't exist
  - **Solution**: Having a validation check on such fields and monitoring the percentage of valid values is a great way
  to control this. Depending on the criticality of the field, it's sometimes okay to just discard it, in other cases,
  the full row of data might need to be written to a corrupt-record table for further investigation.
  Working with the data producers in this case is a great way to implement the validation before the data comes into
  the systems to improve DQ levels.
- Missing data
  - **Issue**: Data fields not having any value
  - **Solution**: It is important to understand the root cause of this issue as it could be anywhere from data
  generation to consumption and may indicate deeper problems with some systems. Once identified, there should also be
  some monitoring on percentages if the root cause cannot be addressed.
- Duplicate Data
  - **Issue**: Having multiples of the same data
  - **Solution**: Again, it is important to understand why this is happening. If the source system is an at-least-once
  delivery system then a simple deduplication by key on ingestion would fix it. If the cause is a bad join in the
  pipeline, then a fix has to be implemented and the historical data has to be checked.
